{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataset.dataset import SkinLesion_Dataset, SegExamples\n",
    "from pipeline.preprocessing import SkinLesionPreprocessing\n",
    "from pipeline.feature_extraction import FeaturesExtraction\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer, cohen_kappa_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (15195, 144), val shape: (3796, 144)\n",
      "train shape multi: (5082, 144), val shape multi: (1270, 144)\n"
     ]
    }
   ],
   "source": [
    "# Binary feather files\n",
    "train_f = pd.read_feather(\"../data/binary/train_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "val_f = pd.read_feather(\"../data/binary/val_all_features.f\", columns=None, use_threads=True, storage_options=None)\n",
    "train_shape = train_f.shape\n",
    "val_shape = val_f.shape\n",
    "print(f'train shape: {train_shape}, val shape: {val_shape}')\n",
    "# Multi-class feather files.\n",
    "train_f_mul = pd.read_feather(\"../data/three_class/train_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "val_f_mul = pd.read_feather(\"../data/three_class/val_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "train_shape_m = train_f_mul.shape\n",
    "val_shape_m = val_f_mul.shape\n",
    "print(f'train shape multi: {train_shape_m}, val shape multi: {val_shape_m}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: (15195, 144), df_val: (3796, 144)\n",
      "X_train_new: float32, y_train: (15195,), X_test_new: (3796, 143), y_test: (3796,)\n",
      "X_var: (18991, 91)\n",
      "Performing Feature Selection ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sfm: (18991, 86)\n",
      "X_train_var: (15195, 91), y_train: (15195,), X_test_var: (3796, 91), y_test: (3796,)\n",
      "X_train_new: (15195, 86), y_train: (15195,), X_test_new: (3796, 86), y_test: (3796,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Pre-processing\n",
    "# Binary case\n",
    "train_f.replace({'nevus': 1, 'others': 0}, inplace=True)\n",
    "val_f.replace({'nevus': 1, 'others': 0}, inplace=True)\n",
    "\n",
    "df_train = train_f.iloc[:train_shape[0], :].sample(frac=1, random_state=42)\n",
    "df_val = val_f.iloc[:val_shape[0], :].sample(frac=1, random_state=42)\n",
    "print(f\"df_train: {df_train.shape}, df_val: {df_val.shape}\")\n",
    "\n",
    "X_train, y_train = df_train.iloc[:, :(train_shape[1]-1)].to_numpy(dtype=np.float32), df_train.iloc[:, (train_shape[1]-1)].to_numpy()\n",
    "X_test, y_test = df_val.iloc[:, :(val_shape[1]-1)].to_numpy(dtype= np.float32), df_val.iloc[:, (val_shape[1]-1)].to_numpy()\n",
    "\n",
    "print(f'X_train_new: {X_train.dtype}, y_train: {y_train.shape}, X_test_new: {X_test.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "X_train_f = pd.DataFrame(X_train)\n",
    "X_test_f = pd.DataFrame(X_test)\n",
    "y_train_f = pd.DataFrame(y_train)\n",
    "y_test_f = pd.DataFrame(y_test)\n",
    "frames = [X_train_f, X_test_f]\n",
    "f = [y_train_f, y_test_f]\n",
    "X = pd.concat(frames)\n",
    "y = pd.concat(f)\n",
    "\n",
    "# Feature Selection -- VARIANCE THRESHOLD\n",
    "sel_var = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_var= sel_var.fit_transform(X)\n",
    "print(f'X_var: {X_var.shape}')\n",
    "\n",
    "## Feature Selection -- SelectFromModel\n",
    "sel_sfm = SelectFromModel(LinearSVC(C = 0.09, penalty=\"l1\",  dual=False))\n",
    "print(\"Performing Feature Selection ... \")\n",
    "X_sfm = sel_sfm.fit_transform(X, y)\n",
    "print(f'X_sfm: {X_sfm.shape}')\n",
    "\n",
    "# split dataframe X again into train and test -- VARIANCE THRESHOLD\n",
    "X_train_var = pd.DataFrame(X_var).iloc[:len(X_train_f[0])].to_numpy(dtype=np.float32)\n",
    "X_test_var = pd.DataFrame(X_var).iloc[len(X_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "y_train_new = pd.DataFrame(y).iloc[:len(y_train_f[0])].to_numpy(dtype=np.float32)\n",
    "y_test_new = pd.DataFrame(y).iloc[len(y_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "print(f'X_train_var: {X_train_var.shape}, y_train: {y_train.shape}, X_test_var: {X_test_var.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "# split dataframe X again into train and test SFM\n",
    "X_train_new = pd.DataFrame(X_sfm).iloc[:len(X_train_f[0])].to_numpy(dtype=np.float32)\n",
    "X_test_new = pd.DataFrame(X_sfm).iloc[len(X_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "y_train_new = pd.DataFrame(y).iloc[:len(y_train_f[0])].to_numpy(dtype=np.float32)\n",
    "y_test_new = pd.DataFrame(y).iloc[len(y_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "print(f'X_train_new: {X_train_new.shape}, y_train: {y_train.shape}, X_test_new: {X_test_new.shape}, y_test: {y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  42.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  46.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  46.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  43.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  46.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  38.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  46.2s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  46.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  39.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=50; total time=  54.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.7min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 3.0min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.7min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 3.0min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 3.2min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.9min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  35.8s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  39.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  57.2s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  43.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  48.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  45.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  39.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  44.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  36.5s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=50; total time=  42.0s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.7min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.8min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.7min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=100; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 3.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.9min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 3.2min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.6min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 3.3min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 3.0min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.01, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  33.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  34.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  33.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  34.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  34.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  35.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  33.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  33.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  34.4s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=50; total time=  33.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.5min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=log_loss, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  34.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  34.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.7s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  33.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=50; total time=  34.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.1min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=100; total time= 1.2min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.4min\n",
      "[CV] END classifier__learning_rate=0.1, classifier__loss=exponential, classifier__n_estimators=200; total time= 2.3min\n",
      "TRAIN- The best parameters are {'classifier__learning_rate': 0.1, 'classifier__loss': 'log_loss', 'classifier__n_estimators': 200} with an accuracy of 0.8113\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting -- without feature selection\n",
    "param_grid = {'classifier__loss': [\"log_loss\", \"exponential\"],\n",
    "              'classifier__learning_rate': [0.01, 0.1],\n",
    "              'classifier__n_estimators': [50, 100, 200]}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "classifier = GradientBoostingClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=cv, refit = True, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"TRAIN- The best parameters are %s with an accuracy of %0.4f\"%(grid_search.best_params_, grid_search.best_score_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of train data: 0.8493 Acc: 0.8493583415597236\n",
      "F1 Score of test data: 0.8088 Acc: 0.8090094836670179\n"
     ]
    }
   ],
   "source": [
    "y_train_predicted = grid_search.predict(X_train)\n",
    "y_test_predicted =  grid_search.predict(X_test)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
