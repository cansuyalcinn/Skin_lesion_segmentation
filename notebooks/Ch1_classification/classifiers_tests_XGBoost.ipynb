{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import cv2\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dataset.dataset import SkinLesion_Dataset, SegExamples\n",
    "from pipeline.preprocessing import SkinLesionPreprocessing\n",
    "from pipeline.feature_extraction import FeaturesExtraction\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer, cohen_kappa_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from boruta import BorutaPy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (15195, 144), val shape: (3796, 144)\n",
      "train shape multi: (5082, 144), val shape multi: (1270, 144)\n"
     ]
    }
   ],
   "source": [
    "# Binary feather files\n",
    "train_f = pd.read_feather(\"../data/binary/train_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "val_f = pd.read_feather(\"../data/binary/val_all_features.f\", columns=None, use_threads=True, storage_options=None)\n",
    "train_shape = train_f.shape\n",
    "val_shape = val_f.shape\n",
    "print(f'train shape: {train_shape}, val shape: {val_shape}')\n",
    "# Multi-class feather files.\n",
    "train_f_mul = pd.read_feather(\"../data/three_class/train_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "val_f_mul = pd.read_feather(\"../data/three_class/val_all_feat.f\", columns=None, use_threads=True, storage_options=None)\n",
    "train_shape_m = train_f_mul.shape\n",
    "val_shape_m = val_f_mul.shape\n",
    "print(f'train shape multi: {train_shape_m}, val shape multi: {val_shape_m}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (15195, 203), y_train: (15195,), X_test: (3796, 203), y_test: (3796,)\n",
      "final class balance [TRAIN]: (array([0, 1], dtype=int64), array([7470, 7725], dtype=int64)) [VAL]: (array([0, 1], dtype=int64), array([1865, 1931], dtype=int64))\n",
      "X_var: (18991, 151)\n",
      "Performing Feature Selection ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sfm: (18991, 133)\n",
      "X_train_var: (15195, 151), y_train: (15195,), X_test_var: (3796, 151), y_test: (3796,)\n",
      "X_train_new: (15195, 133), y_train: (15195,), X_test_new: (3796, 133), y_test: (3796,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df_1 = pd.read_feather(\"../data/binary/train_all_feat.f\")\n",
    "train_df_2 = pd.read_feather(\"../data/binary/train_color_local_feat.f\")\n",
    "val_df_1 = pd.read_feather(\"../data/binary/val_all_features.f\")\n",
    "val_df_2 = pd.read_feather(\"../data/binary/val_color_local_feat.f\")\n",
    "\n",
    "train_df = pd.concat([train_df_2.iloc[:,:60], train_df_1], axis=1)\n",
    "val_df = pd.concat([val_df_2.iloc[:, :60], val_df_1], axis=1)\n",
    "\n",
    "train_shape = train_df.shape\n",
    "val_shape = val_df.shape\n",
    "\n",
    "train_df.replace({'nevus': 1, 'others': 0}, inplace=True)\n",
    "val_df.replace({'nevus': 1, 'others': 0}, inplace=True)\n",
    "\n",
    "df_train = train_df.iloc[:train_shape[0], :].sample(frac=1, random_state=42)\n",
    "df_val = val_df.iloc[:val_shape[0], :].sample(frac=1, random_state=42)\n",
    "# df_val\n",
    "\n",
    "X_train, y_train = df_train.iloc[:, :(train_shape[1]-1)].to_numpy(dtype=np.float32), df_train.iloc[:, (train_shape[1]-1)].to_numpy()\n",
    "X_test, y_test = df_val.iloc[:, :(val_shape[1]-1)].to_numpy(dtype= np.float32), df_val.iloc[:, (val_shape[1]-1)].to_numpy()\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}')\n",
    "print(f'final class balance [TRAIN]: {np.unique(y_train, return_counts=True)}', f'[VAL]: {np.unique(y_test, return_counts=True)}')\n",
    "\n",
    "X_train_f = pd.DataFrame(X_train)\n",
    "X_test_f = pd.DataFrame(X_test)\n",
    "y_train_f = pd.DataFrame(y_train)\n",
    "y_test_f = pd.DataFrame(y_test)\n",
    "frames = [X_train_f, X_test_f]\n",
    "f = [y_train_f, y_test_f]\n",
    "X = pd.concat(frames)\n",
    "y = pd.concat(f)\n",
    "\n",
    "# Feature Selection -- VARIANCE THRESHOLD\n",
    "sel_var = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_var= sel_var.fit_transform(X)\n",
    "print(f'X_var: {X_var.shape}')\n",
    "\n",
    "## Feature Selection -- SelectFromModel\n",
    "sel_sfm = SelectFromModel(LinearSVC(C = 0.09, penalty=\"l1\",  dual=False))\n",
    "print(\"Performing Feature Selection ... \")\n",
    "X_sfm = sel_sfm.fit_transform(X, y)\n",
    "print(f'X_sfm: {X_sfm.shape}')\n",
    "\n",
    "# split dataframe X again into train and test -- VARIANCE THRESHOLD\n",
    "X_train_var = pd.DataFrame(X_var).iloc[:len(X_train_f[0])].to_numpy(dtype=np.float32)\n",
    "X_test_var = pd.DataFrame(X_var).iloc[len(X_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "y_train_new = pd.DataFrame(y).iloc[:len(y_train_f[0])].to_numpy(dtype=np.float32)\n",
    "y_test_new = pd.DataFrame(y).iloc[len(y_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "print(f'X_train_var: {X_train_var.shape}, y_train: {y_train.shape}, X_test_var: {X_test_var.shape}, y_test: {y_test.shape}')\n",
    "\n",
    "# split dataframe X again into train and test SFM\n",
    "X_train_new = pd.DataFrame(X_sfm).iloc[:len(X_train_f[0])].to_numpy(dtype=np.float32)\n",
    "X_test_new = pd.DataFrame(X_sfm).iloc[len(X_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "y_train_new = pd.DataFrame(y).iloc[:len(y_train_f[0])].to_numpy(dtype=np.float32)\n",
    "y_test_new = pd.DataFrame(y).iloc[len(y_train_f[0]):].to_numpy(dtype=np.float32)\n",
    "print(f'X_train_new: {X_train_new.shape}, y_train: {y_train.shape}, X_test_new: {X_test_new.shape}, y_test: {y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [40], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m model \u001B[38;5;241m=\u001B[39m RandomForestClassifier()\n\u001B[0;32m     26\u001B[0m feat_selector \u001B[38;5;241m=\u001B[39m BorutaPy(model, n_estimators \u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m---> 27\u001B[0m feat_selector\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m     28\u001B[0m X_train_boruta \u001B[38;5;241m=\u001B[39m feat_selector\u001B[38;5;241m.\u001B[39mtransform(X_train)\n\u001B[0;32m     29\u001B[0m X_test_boruta \u001B[38;5;241m=\u001B[39m feat_selector\u001B[38;5;241m.\u001B[39mtransform(X_test)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\boruta\\boruta_py.py:201\u001B[0m, in \u001B[0;36mBorutaPy.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y):\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03m    Fits the Boruta feature selection with the provided estimator.\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;124;03m        The target values.\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\boruta\\boruta_py.py:285\u001B[0m, in \u001B[0;36mBorutaPy._fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mset_params(random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n\u001B[0;32m    284\u001B[0m \u001B[38;5;66;03m# add shadow attributes, shuffle them and train estimator, get imps\u001B[39;00m\n\u001B[1;32m--> 285\u001B[0m cur_imp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_shadows_get_imps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdec_reg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# get the threshold of shadow importances we will use for rejection\u001B[39;00m\n\u001B[0;32m    288\u001B[0m imp_sha_max \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mpercentile(cur_imp[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mperc)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\boruta\\boruta_py.py:412\u001B[0m, in \u001B[0;36mBorutaPy._add_shadows_get_imps\u001B[1;34m(self, X, y, dec_reg)\u001B[0m\n\u001B[0;32m    410\u001B[0m x_sha \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mapply_along_axis(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_shuffle, \u001B[38;5;241m0\u001B[39m, x_sha)\n\u001B[0;32m    411\u001B[0m \u001B[38;5;66;03m# get importance of the merged matrix\u001B[39;00m\n\u001B[1;32m--> 412\u001B[0m imp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_imp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_cur\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_sha\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[38;5;66;03m# separate importances of real and shadow features\u001B[39;00m\n\u001B[0;32m    414\u001B[0m imp_sha \u001B[38;5;241m=\u001B[39m imp[x_cur_w:]\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\boruta\\boruta_py.py:384\u001B[0m, in \u001B[0;36mBorutaPy._get_imp\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_imp\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y):\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 384\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    386\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPlease check your X and y variable. The provided\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    387\u001B[0m                          \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mestimator cannot be fitted to your data.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:88\u001B[0m, in \u001B[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_with_self\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper_with_self\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wrapper_impl(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:74\u001B[0m, in \u001B[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_impl\u001B[1;34m(obj, *args, **kwargs)\u001B[0m\n\u001B[0;32m     72\u001B[0m usm_iface \u001B[38;5;241m=\u001B[39m _extract_usm_iface(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     73\u001B[0m q, hostargs, hostkwargs \u001B[38;5;241m=\u001B[39m _get_host_inputs(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 74\u001B[0m result \u001B[38;5;241m=\u001B[39m _run_on_device(func, q, obj, \u001B[38;5;241m*\u001B[39mhostargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhostkwargs)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m usm_iface \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(result, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__array_interface__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _copy_to_usm(q, result)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:65\u001B[0m, in \u001B[0;36m_run_on_device\u001B[1;34m(func, queue, obj, *args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m sycl_context(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m queue\u001B[38;5;241m.\u001B[39msycl_device\u001B[38;5;241m.\u001B[39mis_gpu \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     63\u001B[0m                           host_offload_on_fail\u001B[38;5;241m=\u001B[39mhost_offload):\n\u001B[0;32m     64\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_by_obj(obj, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_by_obj(obj, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:53\u001B[0m, in \u001B[0;36m_run_on_device.<locals>.dispatch_by_obj\u001B[1;34m(obj, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdispatch_by_obj\u001B[39m(obj, func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 53\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(obj, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:672\u001B[0m, in \u001B[0;36mRandomForestClassifier.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;129m@support_usm_ndarray\u001B[39m()\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    647\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;124;03m    Build a forest of trees from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    649\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;124;03m    self : object\u001B[39;00m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_fit_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:346\u001B[0m, in \u001B[0;36m_fit_classifier\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    344\u001B[0m _patching_status\u001B[38;5;241m.\u001B[39mwrite_log()\n\u001B[0;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _dal_ready:\n\u001B[1;32m--> 346\u001B[0m     \u001B[43m_daal_fit_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_estimators_\n\u001B[0;32m    350\u001B[0m     \u001B[38;5;66;03m# Decapsulate classes_ attributes\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\Skin_lesion_segmentation\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:248\u001B[0m, in \u001B[0;36m_daal_fit_classifier\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_estimators_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;66;03m# compute\u001B[39;00m\n\u001B[1;32m--> 248\u001B[0m dfc_trainingResult \u001B[38;5;241m=\u001B[39m \u001B[43mdfc_algorithm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# get resulting model\u001B[39;00m\n\u001B[0;32m    251\u001B[0m model \u001B[38;5;241m=\u001B[39m dfc_trainingResult\u001B[38;5;241m.\u001B[39mmodel\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# # Feature Selection -- Boruta Algorithm using XGboost\n",
    "# model = RandomForestClassifier()\n",
    "# feat_selector = BorutaPy(model, n_estimators ='auto', verbose=2, random_state=1, max_iter=5)\n",
    "# feat_selector.fit(X_train, y_train)\n",
    "# X_train_boruta = feat_selector.transform(X_train)\n",
    "# X_test_boruta = feat_selector.transform(X_test)\n",
    "# # print(f\"best feature {feat_selector.ranking_}\")\n",
    "# print(f'X_train_boruta: {X_train_boruta.shape}, y_train: {y_train.shape}, X_test_boruta: {X_test_boruta.shape}, y_test: {y_test.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  13.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  12.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  14.2s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  11.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  12.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  11.9s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  13.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  15.8s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  15.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=50; total time=  15.8s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  25.2s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  25.7s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  32.1s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  28.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  28.3s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  23.8s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  23.0s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  22.6s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  24.4s\n",
      "[CV] END classifier__learning_rate=0.01, classifier__n_estimators=100; total time=  28.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  14.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  13.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  13.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  12.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  11.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  11.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  11.2s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  12.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  12.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=50; total time=  13.1s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  23.4s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  25.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  21.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  23.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  22.5s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  23.0s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  22.3s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  21.9s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  22.8s\n",
      "[CV] END classifier__learning_rate=0.1, classifier__n_estimators=100; total time=  22.1s\n",
      "TRAIN- The best parameters are {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 100} with an accuracy of 0.8205\n"
     ]
    }
   ],
   "source": [
    "# XGBoost -- without feature selection\n",
    "param_grid = {'classifier__learning_rate': [0.01, 0.1],\n",
    "              'classifier__n_estimators': [50, 100]}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "classifier = xgb.XGBClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=cv, refit = True, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"TRAIN- The best parameters are %s with an accuracy of %0.4f\"%(grid_search.best_params_, grid_search.best_score_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of train data: 0.9057 Acc: 0.9057584731819678\n",
      "F1 Score of test data: 0.8099 Acc: 0.8100632244467861\n"
     ]
    }
   ],
   "source": [
    "y_train_predicted = grid_search.predict(X_train)\n",
    "y_test_predicted =  grid_search.predict(X_test)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV 1/2] END classifier__learning_rate=0.01, classifier__n_estimators=300;, score=0.813 total time=  56.9s\n",
      "[CV 2/2] END classifier__learning_rate=0.01, classifier__n_estimators=300;, score=0.819 total time=  52.8s\n",
      "TRAIN- The best parameters are {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 300} with an accuracy of 0.8160\n",
      "F1 Score of train data: 0.8675 Acc: 0.8675222112537019\n",
      "F1 Score of test data: 0.8128 Acc: 0.8129610115911485\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST\n",
    "param_grid = {'classifier__learning_rate': [0.01],\n",
    "              'classifier__n_estimators': [300]\n",
    "              }\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "classifier = xgb.XGBClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=cv, refit = True, verbose = 3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"TRAIN- The best parameters are %s with an accuracy of %0.4f\"%(grid_search.best_params_, grid_search.best_score_))\n",
    "y_train_predicted = grid_search.predict(X_train)\n",
    "y_test_predicted =  grid_search.predict(X_test)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of train data: 0.9923 Acc: 0.9923000987166831\n",
      "F1 Score of test data: 0.8373 Acc: 0.8374604847207587\n"
     ]
    }
   ],
   "source": [
    "# Default xgboost without gridsearch\n",
    "classifier = xgb.XGBClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_train_predicted = pipe.predict(X_train)\n",
    "y_test_predicted =  pipe.predict(X_test)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of train data: 0.9895 Acc: 0.989470220467259\n",
      "F1 Score of test data: 0.8239 Acc: 0.8240252897787145\n"
     ]
    }
   ],
   "source": [
    "# REsults from Varianec threshold method\n",
    "classifier = xgb.XGBClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "pipe.fit(X_train_var, y_train_new)\n",
    "y_train_predicted = pipe.predict(X_train_var)\n",
    "y_test_predicted =  pipe.predict(X_test_var)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train_new,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train_new, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test_new,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test_new, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of train data: 0.9864 Acc: 0.9864429088515959\n",
      "F1 Score of test data: 0.8263 Acc: 0.8263962065331928\n"
     ]
    }
   ],
   "source": [
    "# REsults from select from model method\n",
    "classifier = xgb.XGBClassifier()\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('classifier', classifier)])\n",
    "pipe.fit(X_train_new, y_train_new)\n",
    "y_train_predicted = pipe.predict(X_train_new)\n",
    "y_test_predicted =  pipe.predict(X_test_new)\n",
    "print('F1 Score of train data: %0.4f' %f1_score(y_train_new,y_train_predicted,average='macro'), f'Acc: {accuracy_score(y_train_new, y_train_predicted)}')\n",
    "print('F1 Score of test data: %0.4f' %f1_score(y_test_new,y_test_predicted,average='macro'), f'Acc: {accuracy_score(y_test_new, y_test_predicted)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
