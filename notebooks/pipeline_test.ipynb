{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys; sys.path.insert(0, os.path.abspath(\"../\"))\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset.dataset import SkinLesion_Dataset, SegExamples\n",
    "from pipeline.preprocessing import SkinLesionPreprocessing\n",
    "from pipeline.feature_extraction import FeaturesExtraction\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline example for binary challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init object instances\n",
    "data = SkinLesion_Dataset(class_task='binary')\n",
    "preproc = SkinLesionPreprocessing()\n",
    "cfe = FeaturesExtraction(['global','local']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time test\n",
    "start_time = time.time()\n",
    "\n",
    "# Read model pickle\n",
    "clf = pd.read_pickle('../data/binary/bin_model.pkl') # trained model available at \n",
    "# https://drive.google.com/file/d/1S5VT32gV5G53bgeyts9CxgZEzlFzh1yc/view?usp=share_link\n",
    "\n",
    "# Select 100 random images\n",
    "rand_images = np.random.randint(0, len(data.md_df), 100)\n",
    "\n",
    "# Extract features\n",
    "feats = []\n",
    "for i in tqdm(rand_images, total=len(rand_images)):\n",
    "    image = data[i]['img']\n",
    "    image_p = preproc.preprocess(image)\n",
    "    mask = preproc.get_seg_mask(image_p)\n",
    "    feats.append(cfe.extract_features(image_p, mask))\n",
    "\n",
    "df = pd.DataFrame(feats, columns=cfe.features_names)\n",
    "test_shape = df.shape\n",
    "\n",
    "# Prepare data\n",
    "X_test = df.iloc[:, :(test_shape[1])].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline example for three-class challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need specific trained models that will be ensembled, and we had issues writing the trained pipelines (pickling) and reading them back, it is necessary to run the classifiers training once again. Below in a single cell is the code to do so. It is only necessary to download three models and scalers from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score, roc_auc_score, cohen_kappa_score, make_scorer\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, SVMSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as impipe\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.utils import class_weight\n",
    "from joblib import dump, load\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Read pre-trained models and scalers from https://drive.google.com/drive/folders/1cr_KMZTzZeTXF2hq834ZXEu4X6w9FNqo?usp=share_link\n",
    "\n",
    "clf1 = pd.read_pickle('../data/three_class/models/clf1_svc.pkl') \n",
    "clf1_scaler = pd.read_pickle('../data/three_class/models/clf1_scaler.pkl')\n",
    "pipe_clf1_svc = Pipeline([('scaler', clf1_scaler), ('classifier', clf1)])\n",
    "\n",
    "clf2 = pd.read_pickle('../data/three_class/models/clf2_svc.pkl')\n",
    "clf2_scaler = pd.read_pickle('../data/three_class/models/clf2_scaler.pkl') \n",
    "pipe_clf2_svc = Pipeline([('scaler', clf2_scaler), ('classifier', clf2)])\n",
    "\n",
    "clf3 = pd.read_pickle('../data/three_class/models/clf3_xgb.pkl')\n",
    "clf3_scaler = pd.read_pickle('../data/three_class/models/clf3_scaler.pkl')\n",
    "pipe_clf3_xgb = Pipeline([('scaler', clf3_scaler), ('classifier', clf3)])\n",
    "\n",
    "# Other models running on the go\n",
    "\n",
    "# data preparation\n",
    "train_df_1 = pd.read_feather(\"../data/three_class/train_all_feat.f\")\n",
    "train_df_2 = pd.read_feather(\"../data/three_class/train_color_local_feat.f\")\n",
    "val_df_1 = pd.read_feather(\"../data/three_class/val_all_feat.f\")\n",
    "val_df_2 = pd.read_feather(\"../data/three_class/val_color_local_feat.f\")\n",
    "\n",
    "train_df = pd.concat([train_df_2.iloc[:,:60], train_df_1], axis=1)\n",
    "val_df = pd.concat([val_df_2.iloc[:, :60], val_df_1], axis=1)\n",
    "\n",
    "# move lbp features at the beginning\n",
    "train_df = train_df[ [ col for col in train_df.columns if 'lbp' in col ] + [ col for col in train_df.columns if 'lbp' not in col ] ]\n",
    "val_df = val_df[ [ col for col in val_df.columns if 'lbp' in col ] + [ col for col in val_df.columns if 'lbp' not in col ] ]\n",
    "\n",
    "train_shape = train_df.shape\n",
    "val_shape = val_df.shape\n",
    "\n",
    "train_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "val_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "\n",
    "df_train = train_df.iloc[:train_shape[0], :].sample(frac=1, random_state=42)\n",
    "df_val = val_df.iloc[:val_shape[0], :].sample(frac=1, random_state=42)\n",
    "# df_val\n",
    "\n",
    "X_train, y_train = df_train.iloc[:, :(train_shape[1]-1)].to_numpy(dtype=np.float32), df_train.iloc[:, (train_shape[1]-1)].to_numpy()\n",
    "X_test, y_test = df_val.iloc[:, :(val_shape[1]-1)].to_numpy(dtype= np.float32), df_val.iloc[:, (val_shape[1]-1)].to_numpy()\n",
    "\n",
    "# training model\n",
    "classifier =  RandomForestClassifier(class_weight='balanced', max_depth=100, n_estimators=300, random_state=42)\n",
    "pipe_clf4_rf = Pipeline([('scaler', StandardScaler()), ('classifier', classifier)])\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "pipe_clf4_rf.fit(X_train, y_train)\n",
    "\n",
    "del train_df, val_df, train_shape, val_shape, X_train, y_train, X_test, y_test, classifier\n",
    "\n",
    "train_df_1 = pd.read_feather(\"../data/three_class/train_all_feat.f\")\n",
    "train_df_2 = pd.read_feather(\"../data/three_class/train_color_local_feat.f\")\n",
    "val_df_1 = pd.read_feather(\"../data/three_class/val_all_feat.f\")\n",
    "val_df_2 = pd.read_feather(\"../data/three_class/val_color_local_feat.f\")\n",
    "\n",
    "train_df = pd.concat([train_df_2.iloc[:,:60], train_df_1], axis=1)\n",
    "val_df = pd.concat([val_df_2.iloc[:, :60], val_df_1], axis=1)\n",
    "\n",
    "# move lbp features at the beginning\n",
    "train_df = train_df[ [ col for col in train_df.columns if 'lbp' in col ] + [ col for col in train_df.columns if 'lbp' not in col ] ]\n",
    "val_df = val_df[ [ col for col in val_df.columns if 'lbp' in col ] + [ col for col in val_df.columns if 'lbp' not in col ] ]\n",
    "\n",
    "train_shape = train_df.shape\n",
    "val_shape = val_df.shape\n",
    "\n",
    "train_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "val_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "\n",
    "df_train = train_df.iloc[:train_shape[0], :].sample(frac=1, random_state=42)\n",
    "df_val = val_df.iloc[:val_shape[0], :].sample(frac=1, random_state=42)\n",
    "X_train, y_train = df_train.iloc[:, :(train_shape[1]-1)].to_numpy(dtype=np.float32), df_train.iloc[:, (train_shape[1]-1)].to_numpy()\n",
    "X_test, y_test = df_val.iloc[:, :(val_shape[1]-1)].to_numpy(dtype= np.float32), df_val.iloc[:, (val_shape[1]-1)].to_numpy()\n",
    "\n",
    "over = SMOTE(sampling_strategy={2: 500}, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy={1: 1200, 0: 1000}, random_state=123)\n",
    "steps = [('o', over), ('u', under)]\n",
    "smote_pipe = impipe(steps=steps)\n",
    "# transform the dataset\n",
    "X_train, y_train = smote_pipe.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier =  RandomForestClassifier(class_weight='balanced', max_depth=50, n_estimators=300, random_state=42)\n",
    "pipe_clf5_rf = Pipeline([('scaler', StandardScaler()), ('classifier', classifier)])\n",
    "pipe_clf5_rf.fit(X_train, y_train)\n",
    "\n",
    "del train_df, val_df, train_shape, val_shape, X_train, y_train, X_test, y_test, classifier\n",
    "\n",
    "train_df_1 = pd.read_feather(\"../data/three_class/train_all_feat.f\")\n",
    "train_df_2 = pd.read_feather(\"../data/three_class/train_color_local_feat.f\")\n",
    "val_df_1 = pd.read_feather(\"../data/three_class/val_all_feat.f\")\n",
    "val_df_2 = pd.read_feather(\"../data/three_class/val_color_local_feat.f\")\n",
    "\n",
    "train_df = pd.concat([train_df_2.iloc[:,:60], train_df_1], axis=1)\n",
    "val_df = pd.concat([val_df_2.iloc[:, :60], val_df_1], axis=1)\n",
    "\n",
    "# move lbp features at the beginning\n",
    "train_df = train_df[ [ col for col in train_df.columns if 'lbp' in col ] + [ col for col in train_df.columns if 'lbp' not in col ] ]\n",
    "val_df = val_df[ [ col for col in val_df.columns if 'lbp' in col ] + [ col for col in val_df.columns if 'lbp' not in col ] ]\n",
    "\n",
    "train_bcc = train_df.loc[train_df.label == 'bcc']\n",
    "train_mel = train_df.loc[train_df.label == 'mel']\n",
    "train_scc = train_df.loc[train_df.label == 'scc']\n",
    "\n",
    "train_mel_resamp = resample(train_mel, replace=False, n_samples=1993, random_state=123) \n",
    "train_scc_resamp = resample(train_scc, replace=True, n_samples=1993, random_state=123) \n",
    "train_df = pd.concat([train_bcc, train_mel_resamp, train_scc_resamp])\n",
    "\n",
    "train_shape = train_df.shape\n",
    "val_shape = val_df.shape\n",
    "\n",
    "train_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "val_df.replace({'bcc': 0, 'mel': 1, 'scc': 2}, inplace=True)\n",
    "\n",
    "df_train = train_df.iloc[:train_shape[0], :].sample(frac=1, random_state=42)\n",
    "df_val = val_df.iloc[:val_shape[0], :].sample(frac=1, random_state=42)\n",
    "# df_val\n",
    "\n",
    "X_train, y_train = df_train.iloc[:, :(train_shape[1]-1)].to_numpy(dtype=np.float32), df_train.iloc[:, (train_shape[1]-1)].to_numpy()\n",
    "X_test, y_test = df_val.iloc[:, :(val_shape[1]-1)].to_numpy(dtype= np.float32), df_val.iloc[:, (val_shape[1]-1)].to_numpy()\n",
    "\n",
    "classifier =  ExtraTreesClassifier(class_weight='balanced', max_depth=50, n_estimators=200, random_state=42)\n",
    "pipe_clf6_ert = Pipeline([('scaler', StandardScaler()), ('classifier', classifier)])\n",
    "pipe_clf6_ert.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init object instances\n",
    "data = SkinLesion_Dataset(class_task='three_class')\n",
    "preproc = SkinLesionPreprocessing()\n",
    "cfe = FeaturesExtraction(['global','local'])\n",
    "\n",
    "# Ensemble prediction function\n",
    "def prediction_ensemble(classifiers: dict, X: np.ndarray, weights = [1, 1, 4, 1, 1]):\n",
    "\n",
    "    weights = np.asarray(weights)\n",
    "\n",
    "    results_soft = np.zeros((X.shape[0],3,len(classifiers)))\n",
    "   \n",
    "    for i, clf in enumerate(classifiers.values()): \n",
    "        results_soft[:,:,i] = clf.predict_proba(X)\n",
    "    results_w = results_soft*np.tile(np.array(weights), (results_soft.shape[0], 1))[:, np.newaxis]\n",
    "\n",
    "    y_pred_soft_w = np.argmax(np.mean(results_w, axis=2), axis=1)\n",
    "    \n",
    "    return y_pred_soft_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time test\n",
    "start_time = time.time()\n",
    "\n",
    "# Select 100 random images\n",
    "rand_images = np.random.randint(0, len(data.md_df), 100)\n",
    "\n",
    "# Extract features\n",
    "feats = []\n",
    "for i in tqdm(rand_images, total=len(rand_images)):\n",
    "    image = data[i]['img']\n",
    "    image_p = preproc.preprocess(image)\n",
    "    mask = preproc.get_seg_mask(image_p)\n",
    "    feats.append(cfe.extract_features(image_p, mask))\n",
    "\n",
    "# Prepare data\n",
    "df = pd.DataFrame(feats, columns=cfe.features_names)\n",
    "test_shape = df.shape\n",
    "df = df[ [ col for col in df.columns if 'lbp' in col ] + [ col for col in df.columns if 'lbp' not in col ] ]\n",
    "X_test = df.iloc[:, :(test_shape[1])].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Predict\n",
    "clf_dict = {'clf1_svc': pipe_clf1_svc, 'clf3_xgb': pipe_clf3_xgb, 'clf4_rf': pipe_clf4_rf, 'clf5_rf': pipe_clf5_rf, 'clf6_ert': pipe_clf6_ert}\n",
    "y_pred = prediction_ensemble(clf_dict, X_test)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cad2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0c90d376db34231447b0a346bd9b020b335514f400822d22910f8af73bdac81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
